{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southeast-petersburg",
   "metadata": {},
   "source": [
    "# Train using LR Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affecting-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from image_classification.preprocessors.jitter_rotate_flip import preprocessor\n",
    "from image_classification.datasets.fashion_mnist import FashionMNIST\n",
    "from image_classification.models.vgg import vgg11_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "loose-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-testament",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indoor-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 128\n",
    "data_dir = \"../data/fashion_mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-nursing",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informative-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-canberra",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "featured-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FashionMNIST(is_train=True, data_dir=data_dir, transform=preprocessor)\n",
    "train_dataset, eval_dataset = random_split(dataset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unlimited-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FashionMNIST(is_train=False, data_dir=data_dir, transform=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "equivalent-block",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(eval_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "basic-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "eval_dl = DataLoader(eval_dataset, batch_size*2, shuffle=False)\n",
    "test_dl = DataLoader(test_dataset, batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "absolute-produce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 28, 28]), torch.Size([128]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, labels = next(iter(train_dl))\n",
    "batch.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-omaha",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "elegant-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_dim: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=out_dim)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "amazing-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reported-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(in_channels=1, out_dim=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "close-charlotte",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-moses",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bibliographic-selection",
   "metadata": {
    "papermill": {
     "duration": 0.0428,
     "end_time": "2021-04-18T13:56:12.026304",
     "exception": false,
     "start_time": "2021-04-18T13:56:11.983504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, betas=(0.9,0.999), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1, verbose=False)\n",
    "###Best accuracy 0.9349, tensor(0.9385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "touched-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "examples_seen = 0\n",
    "eval_interval = 100\n",
    "model.train()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "chinese-restoration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch#0 Examples seen: 128\tEval accuracy: 0.100390625\tEval loss: 3.2527753531932833\t\n",
      "Epoch#0 Examples seen: 12928\tEval accuracy: 0.6650390625\tEval loss: 0.9320821195840836\t\n",
      "Epoch#0 Examples seen: 25728\tEval accuracy: 0.69521484375\tEval loss: 0.8198227941989898\t\n",
      "Epoch#0 Examples seen: 38528\tEval accuracy: 0.7244140625\tEval loss: 0.7340614274144173\t\n",
      "Epoch#1 Examples seen: 50176\tEval accuracy: 0.7275390625\tEval loss: 0.7289986088871956\t\n",
      "Epoch#1 Examples seen: 62976\tEval accuracy: 0.7349609375\tEval loss: 0.7206683188676835\t\n",
      "Epoch#1 Examples seen: 75776\tEval accuracy: 0.74873046875\tEval loss: 0.6602572843432426\t\n",
      "Epoch#1 Examples seen: 88576\tEval accuracy: 0.716015625\tEval loss: 0.779403293132782\t\n",
      "Epoch#2 Examples seen: 100224\tEval accuracy: 0.74130859375\tEval loss: 0.7008888222277164\t\n",
      "Epoch#2 Examples seen: 113024\tEval accuracy: 0.74150390625\tEval loss: 0.7007326304912567\t\n",
      "Epoch#2 Examples seen: 125824\tEval accuracy: 0.75146484375\tEval loss: 0.6899392157793045\t\n",
      "Epoch#2 Examples seen: 138624\tEval accuracy: 0.77265625\tEval loss: 0.6155453220009803\t\n",
      "Epoch#3 Examples seen: 150272\tEval accuracy: 0.75224609375\tEval loss: 0.6635483831167222\t\n",
      "Epoch#3 Examples seen: 163072\tEval accuracy: 0.759765625\tEval loss: 0.647976416349411\t\n",
      "Epoch#3 Examples seen: 175872\tEval accuracy: 0.7814453125\tEval loss: 0.5903568781912327\t\n",
      "Epoch#3 Examples seen: 188672\tEval accuracy: 0.78427734375\tEval loss: 0.5927023321390152\t\n",
      "Epoch#4 Examples seen: 200320\tEval accuracy: 0.78447265625\tEval loss: 0.597463758289814\t\n",
      "Epoch#4 Examples seen: 213120\tEval accuracy: 0.8005859375\tEval loss: 0.5480352900922298\t\n",
      "Epoch#4 Examples seen: 225920\tEval accuracy: 0.8013671875\tEval loss: 0.5381011433899403\t\n",
      "Epoch#4 Examples seen: 238720\tEval accuracy: 0.80556640625\tEval loss: 0.5335988536477089\t\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (x, y) in enumerate(train_dl):\n",
    "        examples_seen += batch_size\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i%eval_interval==0:\n",
    "            losses, accuracies = [], []\n",
    "            for x, y in eval_dl:\n",
    "                with torch.no_grad():\n",
    "                    y_pred = model(x)\n",
    "                    loss = criterion(y_pred, y).item()\n",
    "                    accuracy = (y_pred.argmax(axis=1)==y).float().mean().item()\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "            eval_loss = np.mean(losses)\n",
    "            eval_accuracy = np.mean(accuracies)\n",
    "            print(\n",
    "                f\"Epoch#{epoch} \"\n",
    "                f\"Examples seen: {examples_seen}\\t\"\n",
    "                f\"Eval accuracy: {eval_accuracy}\\t\"\n",
    "                f\"Eval loss: {eval_loss}\\t\"\n",
    "            )\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-amateur",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python375jvsc74a57bd0b8616ea29f167952103aaf3891f5a8d38faff850b2dc4af3c9217394862bfe2f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
